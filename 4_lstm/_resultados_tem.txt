## f3 temperatura
------------------------------------------------------------------------------------------
    past_in   = tf.keras.layers.Input(shape=past_shape,   name="past_data")
    future_in = tf.keras.layers.Input(shape=future_shape, name="future_data")

    # Past data 
    past_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(65, return_sequences=False))(past_in)

    # Future
    decoder_lstm = tf.keras.layers.LSTM(4, return_sequences=False)(future_in)

    # Combine the outputs of encoder and decoder (you can concatenate or merge them)
    future_residue = tf.keras.layers.Flatten()(future_in)
    merged = tf.keras.layers.concatenate([past_lstm, decoder_lstm, future_residue])

    # Final output layer
    #merged = tf.keras.layers.Dense(4* output_units)(merged)
    outputs = tf.keras.layers.Dense(target_dim)(merged)

    model = tf.keras.Model(inputs=[past_in, future_in], outputs=outputs)
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss="mse")
------------------------------------------------------------------------------------------------
min val_loss: 2849 (con densa = 2; pero da peor media)

Sin capa densa extra
Average val_loss over 10 runs: 0.029085 ± 0.000234
Minimum val_loss over 10 runs: 0.028746

Capa densa extra * 4
Average val_loss over 10 runs: 0.028924 ± 0.000218
Minimum val_loss over 10 runs: 0.028571

****************************************
Capa densa dps lstm a 65 

Average val_loss over 10 runs: 0.028834 ± 0.000224
Minimum val_loss over 10 runs: 0.028339

OJO no lo he podido replicar; muy aleatorio
*****************************************
 
## f6 temperatura
    past_data_layer = tf.keras.layers.Input(shape=past_data_shape, name="past_data")
    encoder_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(65, return_sequences=False))(past_data_layer)

    # Decoder part (LSTM for future exogenous features)
    future_data_layer = tf.keras.layers.Input(shape=future_data_shape, name="future_data")
    decoder_lstm = tf.keras.layers.LSTM(4, return_sequences=False)(future_data_layer)

    # Combine the outputs of encoder and decoder (you can concatenate or merge them)
    future_residue = tf.keras.layers.Flatten()(future_data_layer)
    merged = tf.keras.layers.concatenate([encoder_lstm, decoder_lstm, future_residue])
    #merged = tf.keras.layers.concatenate([encoder_lstm, decoder_lstm])
    
    # Final output layer
    merged = tf.keras.layers.Dense(4* output_units)(merged) 
    outputs = tf.keras.layers.Dense(output_units)(merged)

    model = tf.keras.Model(inputs=[past_data_layer, future_data_layer], outputs=outputs)


Average val_loss over 6 runs: 0.045904 ± 0.000183
Minimum val_loss over 6 runs: 0.045578

// LSTM 69
Average val_loss over 6 runs: 0.045999 ± 0.000303
Minimum val_loss over 6 runs: 0.045648

LSTM 64 tb empeora


/// DENSE despues de lstm a 65
Average val_loss over 6 runs: 0.045820 ± 0.000480
Minimum val_loss over 6 runs: 0.045374

*** DEFINITIVO f6 ***
    past_in   = tf.keras.layers.Input(shape=past_shape,   name="past_data")
    future_in = tf.keras.layers.Input(shape=future_shape, name="future_data")

    # Past data 
    past_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(65, return_sequences=False))(past_in)
    past_lstm = tf.keras.layers.Dense(65)(past_lstm)
    
    # Future
    decoder_lstm = tf.keras.layers.LSTM(4, return_sequences=False)(future_in)

    # Combine the outputs of encoder and decoder (you can concatenate or merge them)
    future_residue = tf.keras.layers.Flatten()(future_in)
    merged = tf.keras.layers.concatenate([past_lstm, decoder_lstm, future_residue])

    # Final output layer
    #merged = tf.keras.layers.Dense(4* output_units)(merged)
    outputs = tf.keras.layers.Dense(target_dim)(merged)
    
    model = tf.keras.Model(inputs=[past_data_layer, future_data_layer], outputs=outputs)
