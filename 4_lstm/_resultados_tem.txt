## f3 temperatura
------------------------------------------------------------------------------------------
    past_in   = tf.keras.layers.Input(shape=past_shape,   name="past_data")
    future_in = tf.keras.layers.Input(shape=future_shape, name="future_data")

    # Past data 
    past_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(65, return_sequences=False))(past_in)

    # Future
    decoder_lstm = tf.keras.layers.LSTM(4, return_sequences=False)(future_in)

    # Combine the outputs of encoder and decoder (you can concatenate or merge them)
    future_residue = tf.keras.layers.Flatten()(future_in)
    merged = tf.keras.layers.concatenate([past_lstm, decoder_lstm, future_residue])

    # Final output layer
    #merged = tf.keras.layers.Dense(4* output_units)(merged)
    outputs = tf.keras.layers.Dense(target_dim)(merged)

    model = tf.keras.Model(inputs=[past_in, future_in], outputs=outputs)
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss="mse")
------------------------------------------------------------------------------------------------
min val_loss: 2849 (con densa = 2; pero da peor media)

Sin capa densa extra
Average val_loss over 10 runs: 0.029085 ± 0.000234
Minimum val_loss over 10 runs: 0.028746

Capa densa extra * 4
Average val_loss over 10 runs: 0.028924 ± 0.000218
Minimum val_loss over 10 runs: 0.028571

****************************************
Capa densa dps lstm a 65 

Average val_loss over 10 runs: 0.028834 ± 0.000224
Minimum val_loss over 10 runs: 0.028339

OJO no lo he podido replicar; muy aleatorio
*****************************************
 
## f6 temperatura
    past_data_layer = tf.keras.layers.Input(shape=past_data_shape, name="past_data")
    encoder_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(65, return_sequences=False))(past_data_layer)

    # Decoder part (LSTM for future exogenous features)
    future_data_layer = tf.keras.layers.Input(shape=future_data_shape, name="future_data")
    decoder_lstm = tf.keras.layers.LSTM(4, return_sequences=False)(future_data_layer)

    # Combine the outputs of encoder and decoder (you can concatenate or merge them)
    future_residue = tf.keras.layers.Flatten()(future_data_layer)
    merged = tf.keras.layers.concatenate([encoder_lstm, decoder_lstm, future_residue])
    #merged = tf.keras.layers.concatenate([encoder_lstm, decoder_lstm])
    
    # Final output layer
    merged = tf.keras.layers.Dense(4* output_units)(merged) 
    outputs = tf.keras.layers.Dense(output_units)(merged)

    model = tf.keras.Model(inputs=[past_data_layer, future_data_layer], outputs=outputs)

** NOTA ** VALORES SIN USAR LA DENSA *4 FINAL
Average val_loss over 6 runs: 0.045904 ± 0.000183
Minimum val_loss over 6 runs: 0.045578

// LSTM 69
Average val_loss over 6 runs: 0.045999 ± 0.000303
Minimum val_loss over 6 runs: 0.045648

LSTM 64 tb empeora


/// DENSE despues de lstm a 65
Average val_loss over 6 runs: 0.045820 ± 0.000480
Minimum val_loss over 6 runs: 0.045374

*** DENSA FINAL ***

*** DEFINITIVO f6 ***
    past_in   = tf.keras.layers.Input(shape=past_shape,   name="past_data")
    future_in = tf.keras.layers.Input(shape=future_shape, name="future_data")

    # Past data 
    past_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(65, return_sequences=False))(past_in)
    past_lstm = tf.keras.layers.Dense(65)(past_lstm)
    
    # Future
    decoder_lstm = tf.keras.layers.LSTM(4, return_sequences=False)(future_in)

    # Combine the outputs of encoder and decoder (you can concatenate or merge them)
    future_residue = tf.keras.layers.Flatten()(future_in)
    merged = tf.keras.layers.concatenate([past_lstm, decoder_lstm, future_residue])

    # Final output layer
    merged = tf.keras.layers.Dense(4* target_dim)(merged)
    outputs = tf.keras.layers.Dense(target_dim)(merged)
    
    model = tf.keras.Model(inputs=[past_data_layer, future_data_layer], outputs=outputs)
#############################################################################################
###############################################################
f12
    past_in   = tf.keras.layers.Input(shape=past_shape,   name="past_data")
    future_in = tf.keras.layers.Input(shape=future_shape, name="future_data")

    # Past data 
    past_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(65, return_sequences=False))(past_in)
    past_lstm = tf.keras.layers.Dense(65)(past_lstm)
    
    # Future
    decoder_lstm = tf.keras.layers.LSTM(4, return_sequences=False)(future_in)

    # Combine the outputs of encoder and decoder (you can concatenate or merge them)
    future_residue = tf.keras.layers.Flatten()(future_in)
    merged = tf.keras.layers.concatenate([past_lstm, decoder_lstm, future_residue])

    # Final output layer
    ####### merged = tf.keras.layers.Dense(4* target_dim)(merged)
    outputs = tf.keras.layers.Dense(target_dim)(merged)

    model = tf.keras.Model(inputs=[past_in, future_in], outputs=outputs)
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss="mse")

(=f6; empiezo sin la densa final pq me olvide en f6)
Average val_loss over 6 runs: 0.070450 ± 0.000688
Minimum val_loss over 6 runs: 0.069250

// sin densa despues lstm (solo 1 lstm para pasado, otra fut ,  y 1 densa en concatenate)
Average val_loss over 6 runs: 0.070614 ± 0.000493
Minimum val_loss over 6 runs: 0.069731

// añadir densa final * 4
Average val_loss over 6 runs: 0.070927 ± 0.000869
Minimum val_loss over 6 runs: 0.070135

// densa despues de lstm y densa*4 al final
Average val_loss over 6 runs: 0.070518 ± 0.000421
Minimum val_loss over 6 runs: 0.069980

// lstm + densa  con 69 ambas (y solo 1 densa final) 
Average val_loss over 6 runs: 0.069672 ± 0.000462
Minimum val_loss over 6 runs: 0.069123

// ambas 73 
Average val_loss over 6 runs: 0.070571 ± 0.000679
Minimum val_loss over 6 runs: 0.069540

// ambas 71
Average val_loss over 6 runs: 0.070023 ± 0.000542
Minimum val_loss over 6 runs: 0.069581

// ambas 67
