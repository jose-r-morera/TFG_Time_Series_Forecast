\section{Motivación del proyecto}
La predicción de series temporales es un campo de estudio que ha cobrado gran relevancia en los últimos años, especialmente en el ámbito del aprendizaje automático y el aprendizaje profundo. 
La capacidad de anticipar eventos futuros a partir de datos históricos es fundamental en diversas áreas, como la economía, la meteorología, la salud y la ingeniería. 

El campo de la meteorología, en particular, ha aumentado su relevancia en los útlimos años debido al aumento de fenómenos climáticos extremos y su impacto en la sociedad. 
Prever con antelación la evolución de variables meteorológicas no solo permite planificar recursos, sino también contribuir a la prevención de desastres naturales.

En este contexto, el uso de redes neuronales ha demostrado ser una herramienta poderosa para abordar problemas complejos de predicción.
Gracias a los avances en capacidad de cómputo y la disponibilidad de grandes volúmenes de datos, arquitecturas como LSTM, GRU y Transformers se han consolidado como soluciones de alto rendimiento. 
Estas redes no solo capturan patrones temporales de manera eficiente, sino que también pueden adaptarse dinámicamente a cambios en el comportamiento de la serie, mejorando la precisión y la robustez de las predicciones.

Dentro de este ámbito, se ha estudiado extensamente el desarrollo de modelos ajustados para una o varias ubicaciones concretas. Sin embargo, la creación de un modelo generalista,
que pueda adaptarse a diferentes ubicaciones y condiciones climáticas, es un tema de investigación poco explorado e interesante. Con este tipo de modelos se puede 
conseguir, a partir de un sensor básico, generar predicciones meteorológicas para una ubicación cualquiera, que generalmente serán de calidad mayor a las de los modelos numéricos de predicción 
climática usados por los servicios meteorológcios debido a las limitaciones de resolución espacial de estos.

\section{Planteamineto}
En este trabajo se pretende emplear mediciones de múltiples estaciones meteorológicas de la isla de Tenerife con el fin de desarrollar un modelo de predicción climatológica a corto plazo.
Se busca que dicho modelo sea capaz de generalizar más allá de las estaciones de entrenamiento. Esto es, que a partir de mediciones meteorológicas de cualquier origen, el modelo sea capaz de 
generar predicciones a corto plazo (3, 6 o 12 horas) de gran calidad.
Se deciden considerar 3 variables meteorológicas: temperatura, humedad y presión atmosférica.
La elección de estas variables se basa en su relevancia para la predicción del clima y la existencia de registros en las estaciones meteorológicas de Tenerife.

Se establece un especial énfasis en el tratamiento de los datos, estudiándose exhaustivamente diversas técnicas.

La idea principal pasa por la creación de ventanas de información, que son secuencias de datos consecutivos agrupados 

Como parte del trabajo se contempla el despliegue de una infraestructura tecnológica que permita la captura y el procesamiento de datos públicos,
 así como la creación y evaluación de diversos modelos predictivos que extraigan información valiosa a partir de estos datos.
 
Para ello, se emplará el lenguaje de programación Python, que ofrece una amplia gama de bibliotecas y herramientas para el análisis de datos y la 
implementación de modelos de aprendizaje automático.

Así mismo, se plantea desplegar los modelos en un entorno de producción, permitiendo su uso en aplicaciones prácticas. 
De esta forma, cualquier usuario podrá suministrar mediciones de la variable que desee durante las últimas horas y obtener un pronóstico.

\section{Antecedentes y estado del arte}

El uso de redes neuronales y técnicas de deep learning (aprendizaje profundo) para la predicción de series temporales tiene sus orígenes en la década de 1940. Aunque el término deep learning ha ganado relevancia en los últimos años, los conceptos fundamentales y algunas de las técnicas más empleadas han existido desde hace mucho tiempo, evolucionando y perfeccionándose con el avance de la tecnología.
En 1943, Warren McCulloch y Walter Pitts fueron pioneros en desarrollar un modelo computacional para redes neuronales \cite{mcculloch1943}, 
estableciendo las bases de lo que hoy conocemos como inteligencia artificial basada en redes neuronales. 
Más adelante, en 1949, Donald Hebb introdujo la famosa Teoría Hebbiana, una hipótesis que proponía un mecanismo de aprendizaje basado en la plasticidad neuronal \cite{hebb1949}. 
Este principio se aplicó a modelos computacionales, impulsando la investigación en la simulación del aprendizaje humano.

En 1958, Frank Rosenblatt avanzó en este campo con la creación del perceptrón \cite{rosenblatt1958}, un algoritmo pionero de reconocimiento de patrones basado en una red de aprendizaje de computadora de dos capas.
Este modelo simple operaba mediante operaciones de adición y sustracción, sentando las bases para el desarrollo de redes neuronales más complejas.
Durante las siguientes décadas, la investigación en redes neuronales experimentó un estancamiento debido a limitaciones técnicas y a la falta de métodos eficaces para el entrenamiento de redes con múltiples capas. 
Sin embargo, en 1982, un gran avance revitalizó este campo: la introducción del algoritmo de propagación hacia atrás (backpropagation) \cite{werbos1982}, que resolvía el problema del entrenamiento eficiente de redes neuronales profundas, 
permitiendo la formación de redes multicapa de manera más rápida y eficaz.

La aplicación de redes neuronales profundas a la predicción de series temporales experimentó un impulso decisivo con la introducción de las redes recurrentes (RNN)
a principios de los años ochenta. En 1987, Jeffrey Elman describió un modelo de red recurrente simple capaz de aprender dependencias temporales mediante la alimentación
de su propia salida en la capa oculta al instante siguiente\cite{elman1990}. Sin embargo, estas primeras RNN adolecían del problema del desvanecimiento y explosión del gradiente, lo
que limitaba su eficacia para secuencias largas.

Para superar estas limitaciones, Hochreiter y Schmidhuber propusieron en 1997 las Long Short-Term Memory (LSTM), una arquitectura RNN que incorpora puertas de entrada,
olvido y salida, permitiendo el flujo de gradiente a través de largas secuencias y mejorando sustancialmente la capacidad de memoria de la red \cite{hochreiter1997}.
Posteriormente, Cho et al. introdujeron en 2014 las Gated Recurrent Units (GRU), una simplificación de las LSTM con menor número de parámetros,
que ha demostrado en muchos casos un rendimiento comparable acelerando el entrenamiento[7]. 
Más recientemente, la arquitectura Transformer, basada completamente en mecanismos de atención y sin conexiones recurrentes, revolucionó el campo al
mejorar la paralelización y capturar dependencias de largo alcance en el tiempo con mayor eficacia[8].

A partir de 2015 surgieron estudios que adaptaron estas arquitecturas al ámbito meteorológico. Shi et al. desarrollaron el ConvLSTM, que añade convoluciones espaciales a
la LSTM para modelar simultáneamente la dinámica espacial y temporal en tareas de nowcasting de precipitación[9]. Weyn et al. demostraron en 2019 que redes
profundas convolucionales aplicadas a datos globales de variables meteorológicas pueden mejorar la predicción a corto plazo frente a los modelos numéricos tradicionales,
especialmente al resolver patrones locales que escapan a la resolución de la rejilla de los sistemas de predicción numérica del tiempo (NWP)[10].

La mayoría de los enfoques anteriores se especializan en una región o estación concreta, entrenando modelos independientes para cada ubicación. Esto requiere un esfuerzo de
reentrenamiento y calibración local cada vez que se desea aplicar el sistema en un punto nuevo. Algunos trabajos recientes abordan la generalización geográfica mediante la
 inclusión de información posicional o condiciones iniciales específicas de cada punto dentro del modelo, aunque todavía con resultados preliminares[11][12]. 
El desarrollo de un modelo verdaderamente “generalista” —capaz de recibir como entrada únicamente datos de sensores básicos, sin necesidad de reentrenar, y proporcionar predicciones meteorológicas
de calidad comparable o superior a las de los NWP en cualquier ubicación— se conoce como el problema de zero-shot, debido 
y constituye un reto abierto y de gran interés práctico.

\section{Objetivos}

\begin{enumerate}[label=\arabic*.] 
    \item Evaluar y seleccionar una fuente de datos.
    \item Evaluar y seleccionar el framework de aprendizaje profundo a usar en el proyecto.
    \item Diseño e implementación de la metodología de procesamiento de los datos.
    \item Evaluación y selección de arquitecturas pertinentes al problema.
    \item Implementación de una arquitectura adecuada.
    \item Evaluación del rendimiento de la solución propuesta.
    \item Comparación frente a metodologías tradicionales.
    \item Despliegue de la solución en un entorno de producción.
\end{enumerate}

\newpage
