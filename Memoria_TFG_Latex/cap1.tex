\section{Motivación del proyecto}
\subsection{Apartado Uno}
\begin{large}
Texto del apartado uno
As shown in \cite{smith2023}, the results are promising.

\begin{itemize}
   \item Item 1
   \item Item 2
   \item Item 3
   \item Item 4
\end{itemize}
\end{large}

\section{Planteamineto}
Este trabajo consiste en el despliegue de una infraestructura tecnológica para la captura y el procesamiento de datos públicos, 
junto con el diseño, desarrollo y validación de un modelo predictivo que permita extraer información valiosa a partir de dichos datos.
Este enfoque integrará tanto aspectos técnicos relacionados con la gestión y el análisis de datos como la aplicación de técnicas predictivas
basadas en modelos de aprendizaje automático, con el fin de optimizar y automatizar la toma de decisiones en un contexto específico.
\begin{large}
\begin{itemize}
   \item Item 1
   \item Item 2
   \item Item 3
\end{itemize}
\end{large}

\section{Antecedentes y estado del arte}

El uso de redes neuronales y técnicas de deep learning para la predicción de series temporales tiene sus orígenes en la década de 1940. Aunque el término deep learning ha ganado relevancia en los últimos años, los conceptos fundamentales y algunas de las técnicas más empleadas han existido desde hace mucho tiempo, evolucionando y perfeccionándose con el avance de la tecnología.
En 1943, Warren McCulloch y Walter Pitts fueron pioneros en desarrollar un modelo computacional para redes neuronales, estableciendo las bases de lo que hoy conocemos como inteligencia artificial basada en redes neuronales. Más adelante, en 1949, Donald Hebb introdujo la famosa Teoría Hebbiana, una hipótesis que proponía un mecanismo de aprendizaje basado en la plasticidad neuronal. Este principio se aplicó a modelos computacionales, impulsando la investigación en la simulación del aprendizaje humano.


En 1958, Frank Rosenblatt avanzó en este campo con la creación del perceptrón, un algoritmo pionero de reconocimiento de patrones basado en una red de aprendizaje de computadora de dos capas. Este modelo simple operaba mediante operaciones de adición y sustracción, sentando las bases para el desarrollo de redes neuronales más complejas.
Durante las siguientes décadas, la investigación en redes neuronales experimentó un estancamiento debido a limitaciones técnicas y a la falta de métodos eficaces para el entrenamiento de redes con múltiples capas. Sin embargo, en 1982, un gran avance revitalizó este campo: la introducción del algoritmo de propagación hacia atrás (backpropagation), que resolvía el problema del entrenamiento eficiente de redes neuronales profundas, permitiendo la formación de redes multicapa de manera más rápida y eficaz.
Desde entonces, las redes neuronales han continuado evolucionando, impulsadas en gran parte por los avances en la potencia de cálculo de las GPU y el desarrollo del deep learning. Hoy en día, esta tecnología es fundamental y revolucionaria, capaz de realizar predicciones de series temporales con notable rapidez y precisión.
La investigación en este ámbito sigue siendo prolífica. Una búsqueda rápida en internet revela la continua producción de proyectos y estudios enfocados en perfeccionar las técnicas de predicción temporal mediante aprendizaje profundo. Estos trabajos se caracterizan por un análisis exhaustivo y la exploración de nuevos métodos, todos con el objetivo de optimizar la precisión y la eficacia de las previsiones en aplicaciones prácticas.

\section{Objetivos}

\begin{enumerate}[label=\arabic*.] 
    \item Evaluar y seleccionar una fuente de datos.
    \item Evaluar y seleccionar el framework de aprendizaje profundo a usar en el proyecto.
    \item Diseño e implementación de la metodología de procesamiento de los datos.
    \item Evaluación y selección de arquitecturas pertinentes al problema.
    \item Implementación de una arquitectura adecuada.
    \item Evaluación del rendimiento de la solución propuesta.
    \item Comparación frente a metodologías tradicionales.
\end{enumerate}

\newpage
